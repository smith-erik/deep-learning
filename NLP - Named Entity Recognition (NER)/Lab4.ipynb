{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Bidirectional, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "LSTM_UNITS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    " - We replace the one-hot vectors with embeddings.  \n",
    " - Word embeddings are dense vectors obtained by a principal component\n",
    "analysis or another method.  \n",
    " - They can be trained by the neural network or pretained.  \n",
    "   Here, we will use pretrained embeddings from the GloVe project.  \n",
    "   Our version of GloVe is lowercased, so set all the characters in lower case.  \n",
    " - We will add an Embedding layer at the start of the network\n",
    "   We will initialize it with GloVe and make it trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '/Users/erik/Dropbox/Public/Akademiskt/Year5/MachineLearning/Lab4/glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(embeddings_dict['the']))\n",
    "embeddings_dict['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoNLL 2003 - File Structure\n",
    "\n",
    "| Words    | POS | Groups | Named entities |\n",
    "|----------|-----|--------|----------------|\n",
    "| UN       | NNP | I-NP   | I-ORG          |\n",
    "| official | NN  | I-NP   | O              |\n",
    "\n",
    "_Note:  \n",
    "Group detection is also called \"chunking\"._\n",
    "\n",
    "#### Types of named entities:\n",
    "  - persons\n",
    "  - locations\n",
    "  - organizations\n",
    "  - names of miscellaneous entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}]\n",
      "[{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'}, {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'}, {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'}, {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'}, {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'}, {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-DOCSTART- -X- -X- O\\n\\nEU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto T'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = '/Users/erik/Dropbox/Public/Akademiskt/Year5/MachineLearning/Lab4/'\n",
    "\n",
    "def load_conll2003_en():\n",
    "    train_file = BASE_DIR + 'train.txt'\n",
    "    dev_file = BASE_DIR + 'valid.txt'\n",
    "    test_file = BASE_DIR + 'test.txt'\n",
    "    column_names = ['form', 'ppos', 'pchunk', 'ner']\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "    train_dict = conll_dict.transform(train_sentences)\n",
    "    print(train_dict[0])\n",
    "    print(train_dict[1])\n",
    "\n",
    "train_sentences, dev_sentences, test_sentences, column_names = load_conll2003_en()\n",
    "#train_sentences, dev_sentences, test_sentences, column_names = load_ud_en_ewt()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentences, train:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'form': '-DOCSTART-', 'ppos': '-X-', 'pchunk': '-X-', 'ner': 'O'}],\n",
       " [{'form': 'EU', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-ORG'},\n",
       "  {'form': 'rejects', 'ppos': 'VBZ', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'German', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'call', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': 'to', 'ppos': 'TO', 'pchunk': 'B-VP', 'ner': 'O'},\n",
       "  {'form': 'boycott', 'ppos': 'VB', 'pchunk': 'I-VP', 'ner': 'O'},\n",
       "  {'form': 'British', 'ppos': 'JJ', 'pchunk': 'B-NP', 'ner': 'B-MISC'},\n",
       "  {'form': 'lamb', 'ppos': 'NN', 'pchunk': 'I-NP', 'ner': 'O'},\n",
       "  {'form': '.', 'ppos': '.', 'pchunk': 'O', 'ner': 'O'}],\n",
       " [{'form': 'Peter', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-PER'},\n",
       "  {'form': 'Blackburn', 'ppos': 'NNP', 'pchunk': 'I-NP', 'ner': 'I-PER'}],\n",
       " [{'form': 'BRUSSELS', 'ppos': 'NNP', 'pchunk': 'B-NP', 'ner': 'B-LOC'},\n",
       "  {'form': '1996-08-22', 'ppos': 'CD', 'pchunk': 'I-NP', 'ner': 'O'}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep=' +')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "dev_dict = conll_dict.transform(dev_sentences)\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentences, train:\\n')\n",
    "train_dict[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a list of *lists containing maps*.\n",
    "  - The map is one word with its tags.\n",
    "  - The *inner lists are sentences*, i.e. a collection of words.\n",
    "  - Finally, we have an outer list since we have a whole bunch of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Corpus and Building Indices\n",
    "You will read the corpus with programs available from https://github.com/pnugues/edan95. These programs will enable you to load the files in the form of a list of dictionaries.\n",
    "\n",
    "  1. Write a function that for each sentence returns the $\\mathbf{X}$  and $\\mathbf{Y}$ lists of symbols consisting of _words_ and _NER tags_.\n",
    "  2. Create a vocabulary of all the words observed in the training set and the words in GloVe.\n",
    "  2. Create indices and inverted indices for the words and the NER: i.e. you will associate each word with a number. You will use index 0 for the padding symbol and 1 for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a function that for each sentence returns the X and Y lists of symbols consisting of words and NER tags.\n",
    "def build_sequences(corpus_dict, key_x='form', key_y='ner', tolower=True):\n",
    "    \"\"\"Creates sequences from a list of dictionaries.\"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in corpus_dict:\n",
    "        x = []\n",
    "        y = []\n",
    "        for word in sentence:\n",
    "            x += [word[key_x]]\n",
    "            y += [word[key_y]]\n",
    "        if tolower:\n",
    "            x = list(map(str.lower, x))\n",
    "        X += [x]\n",
    "        Y += [y]\n",
    "    return X, Y\n",
    "\n",
    "X_train_cat, Y_train_cat = build_sequences(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-docstart-'],\n",
       " ['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " ['peter', 'blackburn'],\n",
       " ['brussels', '1996-08-22']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sentences, X:words\n",
    "X_train_cat[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O'],\n",
       " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       " ['B-PER', 'I-PER'],\n",
       " ['B-LOC', 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With corresponding tags, Y:POS\n",
    "Y_train_cat[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a vocabulary of all the words observed in the training set and the words in GloVe.\n",
    "vocabulary_words = sorted(list(\n",
    "    set([word for sentence \n",
    "         in X_train_cat for word in sentence])))\n",
    "ner = sorted(list(set([ner for sentence \n",
    "                       in Y_train_cat for ner in sentence])))\n",
    "NB_CLASSES = len(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "Length 9.\n",
      "\n",
      "['goinxha', 'golan', 'gold', 'golden', 'goldiman', 'goldiner', 'goldman', 'golds', 'goldsmith', 'goldstein']\n",
      "Length 21010.\n"
     ]
    }
   ],
   "source": [
    "print(\"%s\\nLength %d.\\n\" % (ner, len(ner)) )\n",
    "print(\"%s\\nLength %d.\" % (vocabulary_words[10000:10010] , len(vocabulary_words)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create indices and inverted indices for the words and the NER.  \n",
    "**You will associate each word with a number.**  \n",
    "You will use index *0 for the padding symbol* and *1 for unknown words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in GloVe: 400000\n",
      "Number of unique words in the vocabulary; embeddings and corpus: 402597\n"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = sorted(list(set(vocabulary_words + \n",
    "                                   list(embeddings_words))))\n",
    "cnt_uniq = len(vocabulary_words) + 2\n",
    "print('Number of unique words in the vocabulary; embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index:\n",
      " [('chetwynd-talbot', 100002), ('cheuk', 100003), ('cheul', 100004), ('cheula', 100005), ('cheun', 100006), ('cheung', 100007), ('cheuse', 100008), ('cheuvreux', 100009), ('cheuvront', 100010), ('chev', 100011)]\n",
      "\n",
      "Ner Index:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 2,\n",
       " 'B-MISC': 3,\n",
       " 'B-ORG': 4,\n",
       " 'B-PER': 5,\n",
       " 'I-LOC': 6,\n",
       " 'I-MISC': 7,\n",
       " 'I-ORG': 8,\n",
       " 'I-PER': 9,\n",
       " 'O': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We start at one to make provision for the padding symbol 0 in RNN and LSTMs and 1 for the unknown words.\n",
    "# This means we have to start enumeration from 2.\n",
    "rev_word_idx = dict(enumerate(vocabulary_words, start=2))\n",
    "rev_ner_idx = dict(enumerate(ner, start=2))\n",
    "\n",
    "# Flip order of values and keys.\n",
    "word_idx = {v: k for k, v in rev_word_idx.items()}\n",
    "ner_idx = {v: k for k, v in rev_ner_idx.items()}\n",
    "\n",
    "print('Word Index:\\n', list(word_idx.items())[100000:100010])\n",
    "print('\\nNer Index:')\n",
    "\n",
    "ner_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the X and Y Sequences\n",
    "You will now create the input and output sequences with numerical indices\n",
    "\n",
    "Convert the X and Y list of symbols in a list of numbers using the indices you created.\n",
    "Pad the sentences using the pad_sequences function.\n",
    "Do the same for the development set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note_\n",
    "\n",
    "`get(key[, default])`  \n",
    "Return the value for key if key is in the dictionary, else default. If default is not given, it defaults to None, so that this method never raises a KeyError.\n",
    "\n",
    "\n",
    "Also, the `map()` function executes a specified function for each item in a iterable. The item is sent to the function as a parameter.  \n",
    "\\> `map(function, iterables)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(X, idx):\n",
    "    \"\"\"\n",
    "    Convert the word/NER-tag lists to indexes.\n",
    "    :param X: List of word/NER-tag lists\n",
    "    :param idx: word to number dictionary\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_idx = []\n",
    "    for x in X:\n",
    "        # We map the unknown words to one\n",
    "        x_idx = list(map(lambda x: idx.get(x, 1), x)) # So set to 1 (our unknown indicator) if not found in idx.\n",
    "        X_idx += [x_idx]\n",
    "    return X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the parallel sequences of indexes, i.e. sentences as indices.\n",
    "X_idx = to_index(X_train_cat, word_idx)\n",
    "Y_idx = to_index(Y_train_cat, ner_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence in X_idx: Word Indices.\n",
      "[340921, 517, 73883, 15424]\n",
      "A sentence in Y_idx: NER Indices.\n",
      "[2, 10, 2, 10]\n"
     ]
    }
   ],
   "source": [
    "print('A sentence in X_idx: Word Indices.')\n",
    "print(X_idx[2001])\n",
    "\n",
    "print('A sentence in Y_idx: NER Indices.')\n",
    "print(Y_idx[2001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note tags corresponding to words:  \n",
    "`{'B-LOC': 2,\n",
    " 'B-MISC': 3,\n",
    " 'B-ORG': 4,\n",
    " 'B-PER': 5,\n",
    " 'I-LOC': 6,\n",
    " 'I-MISC': 7,\n",
    " 'I-ORG': 8,\n",
    " 'I-PER': 9,\n",
    " 'O': 10}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remember: `from keras.preprocessing.sequence import pad_sequences`*  \n",
    "https://keras.io/preprocessing/sequence/#pad_sequences\n",
    "\n",
    "`keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)`\n",
    "\n",
    ">Pads sequences to the same length.  \n",
    "\n",
    ">This function transforms a list of `num_samples` sequences (*lists of integers*) into a 2D Numpy array of shape `(num_samples, num_timesteps)`. `num_timesteps` is either the `maxlen` argument if provided, *or the length of the longest sequence otherwise*.  \n",
    " Sequences that are shorter than `num_timesteps` are padded with `value` at the end.  \n",
    " Sequences longer than `num_timesteps` are truncated so that they fit the desired length. The position where padding or truncation happens is determined by the arguments padding and truncating, respectively.  \n",
    " Pre-padding is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence above is padded into:\n",
      "\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0 340921\n",
      "    517  73883  15424]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  2 10  2 10]\n"
     ]
    }
   ],
   "source": [
    "# Since maxlen not provided, will pre-pad to length of longest sequence: 113 in this case.\n",
    "X = pad_sequences(X_idx)\n",
    "Y = pad_sequences(Y_idx)\n",
    "print('Sentence above is padded into:\\n')\n",
    "print(X[2001])\n",
    "print(Y[2001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: `from keras.utils import to_categorical`  \n",
    "https://keras.io/utils/#to_categorical\n",
    "\n",
    "`keras.utils.to_categorical(y, num_classes=None, dtype='float32')`\n",
    "\n",
    "> Converts a class vector (integers) to binary class matrix.  \n",
    "  E.g. for use with categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: second to last word in padded sentence above.\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Example: the whole padded sentence above.\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      "Total number of sentences: 14987.\n",
      "Padded sentence length is: 113.\n",
      "Each word in sentence can be this number of categories: 11.\n"
     ]
    }
   ],
   "source": [
    "# Encode categories (integer) vector with a binary class matrix.\n",
    "Y_train = to_categorical(Y, num_classes=len(ner) + 2) # The +2 is still a bit mysterious....\n",
    "\n",
    "print(\"Example: second to last word in padded sentence above.\")\n",
    "print(Y_train[2001][-2])\n",
    "\n",
    "print(\"\\nExample: the whole padded sentence above.\")\n",
    "print(Y_train[2001])\n",
    "\n",
    "print(\"\\nTotal number of sentences: %d.\" % len(Y_train))\n",
    "print(\"Padded sentence length is: %d.\" % len(Y_train[2001]))\n",
    "print(\"Each word in sentence can be this number of categories: %d.\" % len(Y_train[2001][-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process validation input too\n",
    "\n",
    "Since we use a validation (development) set, we need to preprocess it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences and tags: 3466, 3466.\n",
      "Padded to length: 109. \n",
      "\n",
      "Example, sentence at index 2001:\n",
      "\n",
      "X: Words Indices\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0  23815 312139  53251    159  64830    325      1]\n",
      "\n",
      "Y: Tag indices\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 10  5  9 10  2 10 10]\n",
      "\n",
      "Example tags with sentence above:\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "Check shape consistency:\n",
      "(3466, 109)\n",
      "(3466, 109, 11)\n"
     ]
    }
   ],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "X_val_cat, Y_val_cat = build_sequences(dev_dict)\n",
    "# We create the parallel sequences of indexes\n",
    "X_val_idx = to_index(X_val_cat, word_idx)\n",
    "Y_val_idx = to_index(Y_val_cat, ner_idx)\n",
    "\n",
    "X_val_padded = pad_sequences(X_val_idx)\n",
    "Y_val_padded = pad_sequences(Y_val_idx)\n",
    "print('Number of test sentences and tags: %d, %d.' % (len(X_val_padded), len(Y_val_padded)) )\n",
    "print('Padded to length: %d.' % len(X_val_padded[2001]), '\\n')\n",
    "\n",
    "print('Example, sentence at index 2001:\\n')\n",
    "print('X: Words Indices\\n', X_val_padded[2001])\n",
    "print('\\nY: Tag indices\\n', Y_val_padded[2001])\n",
    "\n",
    "# Encode categories (integer) vector with a binary class matrix.\n",
    "# One extra symbol for 0 (padding).\n",
    "Y_val_padded_vectorized = to_categorical(Y_val_padded, num_classes=len(ner) + 2)\n",
    "\n",
    "print('\\nExample tags with sentence above:')\n",
    "print(Y_val_padded_vectorized[2001][-10:])\n",
    "\n",
    "print('\\nCheck shape consistency:')\n",
    "print(X_val_padded.shape)\n",
    "print(Y_val_padded_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the embedding matrix\n",
    "1. Create a matrix whose size will be that of all the words: The unique words in the training set and the words in GloVe. Initialize it with random values.\n",
    "2. Fill the matrix with the GloVe embeddings. You will use the indices from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402597, 100)\n"
     ]
    }
   ],
   "source": [
    "rdstate = np.random.RandomState(1234567)\n",
    "embedding_matrix = rdstate.uniform(-0.05, 0.05, \n",
    "                                   (len(vocabulary_words) + 2, \n",
    "                                    EMBEDDING_DIM))\n",
    "print(np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value.\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check with output below\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Check with output below')\n",
    "embeddings_dict['the'][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (402597, 100) \n",
      "\n",
      "Embedding of word \"the\"\n",
      " [-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
      "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
      " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
      "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
      " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
      "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
      "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
      " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
      " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
      " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
      " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
      " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
      " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
      " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
      "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
      " -0.52028    -0.1459      0.82779998  0.27061999] \n",
      "\n",
      "Embedding of the padding symbol, idx 0, random numbers\n",
      " [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453] \n",
      "\n",
      "Embedding of unknown word, idx 1, random numbers\n",
      " [-0.02629708 -0.04923516 -0.04801697 -0.01869074 -0.04005453]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of embedding matrix:', embedding_matrix.shape, '\\n')\n",
    "\n",
    "print('Embedding of word \\\"the\\\"\\n', embedding_matrix[word_idx['the']], '\\n')\n",
    "\n",
    "print('Embedding of the padding symbol, idx 0, random numbers\\n', embedding_matrix[0][0:5], '\\n')\n",
    "\n",
    "print('Embedding of unknown word, idx 1, random numbers\\n', embedding_matrix[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/layers/embeddings/\n",
    "\n",
    "```\n",
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "LSTM_UNITS = 512\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = len(vocabulary_words) + 2\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features,\n",
    "                           EMBEDDING_DIM,\n",
    "                           mask_zero=True,\n",
    "                           input_length=None))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "# The default is True\n",
    "model.layers[0].trainable = True\n",
    "#model.add(SimpleRNN(100, return_sequences=True))\n",
    "model.add(Bidirectional(SimpleRNN(100, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)))\n",
    "#model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "#model.add(Bidirectional(LSTM(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(NB_CLASSES + 2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         40259700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 200)         40200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 11)          2211      \n",
      "=================================================================\n",
      "Total params: 40,302,111\n",
      "Trainable params: 40,302,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14987 samples, validate on 3466 samples\n",
      "Epoch 1/30\n",
      "14987/14987 [==============================] - 68s 5ms/step - loss: 0.5076 - acc: 0.8649 - val_loss: 0.3024 - val_acc: 0.9154\n",
      "Epoch 2/30\n",
      "14987/14987 [==============================] - 65s 4ms/step - loss: 0.2702 - acc: 0.9232 - val_loss: 0.1953 - val_acc: 0.9474\n",
      "Epoch 3/30\n",
      "14987/14987 [==============================] - 65s 4ms/step - loss: 0.2180 - acc: 0.9371 - val_loss: 0.1725 - val_acc: 0.9544\n",
      "Epoch 4/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1864 - acc: 0.9464 - val_loss: 0.1516 - val_acc: 0.9601\n",
      "Epoch 5/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1660 - acc: 0.9518 - val_loss: 0.1428 - val_acc: 0.9621\n",
      "Epoch 6/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1504 - acc: 0.9559 - val_loss: 0.1372 - val_acc: 0.9638\n",
      "Epoch 7/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1384 - acc: 0.9591 - val_loss: 0.1328 - val_acc: 0.9651\n",
      "Epoch 8/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1277 - acc: 0.9625 - val_loss: 0.1240 - val_acc: 0.9666\n",
      "Epoch 9/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1181 - acc: 0.9649 - val_loss: 0.1189 - val_acc: 0.9687\n",
      "Epoch 10/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1114 - acc: 0.9673 - val_loss: 0.1176 - val_acc: 0.9673\n",
      "Epoch 11/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.1058 - acc: 0.9687 - val_loss: 0.1140 - val_acc: 0.9693\n",
      "Epoch 12/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0982 - acc: 0.9708 - val_loss: 0.1140 - val_acc: 0.9703\n",
      "Epoch 13/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0948 - acc: 0.9725 - val_loss: 0.1125 - val_acc: 0.9707\n",
      "Epoch 14/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0889 - acc: 0.9739 - val_loss: 0.1145 - val_acc: 0.9712\n",
      "Epoch 15/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0853 - acc: 0.9747 - val_loss: 0.1072 - val_acc: 0.9718\n",
      "Epoch 16/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0820 - acc: 0.9759 - val_loss: 0.1088 - val_acc: 0.9724\n",
      "Epoch 17/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0794 - acc: 0.9762 - val_loss: 0.1071 - val_acc: 0.9723\n",
      "Epoch 18/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0754 - acc: 0.9780 - val_loss: 0.1020 - val_acc: 0.9720\n",
      "Epoch 19/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0722 - acc: 0.9785 - val_loss: 0.1068 - val_acc: 0.9728\n",
      "Epoch 20/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0703 - acc: 0.9792 - val_loss: 0.1014 - val_acc: 0.9716\n",
      "Epoch 21/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0670 - acc: 0.9799 - val_loss: 0.1075 - val_acc: 0.9732\n",
      "Epoch 22/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0658 - acc: 0.9806 - val_loss: 0.0993 - val_acc: 0.9722\n",
      "Epoch 23/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0645 - acc: 0.9807 - val_loss: 0.1053 - val_acc: 0.9737\n",
      "Epoch 24/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0614 - acc: 0.9816 - val_loss: 0.1022 - val_acc: 0.9738\n",
      "Epoch 25/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0594 - acc: 0.9821 - val_loss: 0.1021 - val_acc: 0.9741\n",
      "Epoch 26/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0582 - acc: 0.9824 - val_loss: 0.0989 - val_acc: 0.9726\n",
      "Epoch 27/30\n",
      "14987/14987 [==============================] - 66s 4ms/step - loss: 0.0570 - acc: 0.9828 - val_loss: 0.1014 - val_acc: 0.9742\n",
      "Epoch 28/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0547 - acc: 0.9838 - val_loss: 0.0986 - val_acc: 0.9732\n",
      "Epoch 29/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0534 - acc: 0.9838 - val_loss: 0.0992 - val_acc: 0.9748\n",
      "Epoch 30/30\n",
      "14987/14987 [==============================] - 67s 4ms/step - loss: 0.0530 - acc: 0.9841 - val_loss: 0.0961 - val_acc: 0.9749\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "history = model.fit(X, Y_train, epochs=n_epochs, batch_size=BATCH_SIZE, \n",
    "                    validation_data=(X_val_padded, Y_val_padded_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('simpleRNN_6e.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results on validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1,n_epochs+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Evaluation\n",
    "\n",
    "### Rehsape test input data\n",
    "Need to do the whole input formatting procedure for the **test set** too.\n",
    "\n",
    "So just repeat what we did above but with the test set as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences and tags: 3684, 3684.\n",
      "Padded to length: 124. \n",
      "\n",
      "Example, sentence at index 2001:\n",
      "\n",
      "X: Words Indices\n",
      " [     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0 359698 195584 375978\n",
      " 184172 327134  87571 107473 175799 108345 195756 143910 270183 195625\n",
      "    154  15453  87571    517  56102 195553 390246  59334 359698 241243\n",
      " 270361  89620 321432    936]\n",
      "\n",
      "Y: Tag indices\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0 10  3 10 10  4 10 10 10 10 10 10 10  2 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10]\n",
      "\n",
      "Example tags with sentence above:\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "Check shape consistency:\n",
      "(3684, 124)\n",
      "(3684, 124, 11)\n"
     ]
    }
   ],
   "source": [
    "# In X_dict, we replace the words with their index\n",
    "X_test_cat, Y_test_cat = build_sequences(test_dict)\n",
    "# We create the parallel sequences of indexes\n",
    "X_test_idx = to_index(X_test_cat, word_idx)\n",
    "Y_test_idx = to_index(Y_test_cat, ner_idx)\n",
    "\n",
    "X_test_padded = pad_sequences(X_test_idx)\n",
    "Y_test_padded = pad_sequences(Y_test_idx)\n",
    "print('Number of test sentences and tags: %d, %d.' % (len(X_test_padded), len(Y_test_padded)) )\n",
    "print('Padded to length: %d.' % len(X_test_padded[2001]), '\\n')\n",
    "\n",
    "print('Example, sentence at index 2001:\\n')\n",
    "print('X: Words Indices\\n', X_test_padded[2001])\n",
    "print('\\nY: Tag indices\\n', Y_test_padded[2001])\n",
    "\n",
    "# Encode categories (integer) vector with a binary class matrix.\n",
    "# One extra symbol for 0 (padding).\n",
    "Y_test_padded_vectorized = to_categorical(Y_test_padded,num_classes=len(ner) + 2)\n",
    "\n",
    "print('\\nExample tags with sentence above:')\n",
    "print(Y_test_padded_vectorized[2001][-15:-10])\n",
    "\n",
    "print('\\nCheck shape consistency:')\n",
    "print(X_test_padded.shape)\n",
    "print(Y_test_padded_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 3s 903us/step\n",
      "Loss: 0.14943445232331784\n",
      "Accuracy: 0.960675511440418\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_padded, \n",
    "                                     Y_test_padded_vectorized)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_padded [     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0 359698 195584 375978\n",
      " 184172 327134  87571 107473 175799 108345 195756 143910 270183 195625\n",
      "    154  15453  87571    517  56102 195553 390246  59334 359698 241243\n",
      " 270361  89620 321432    936]\n",
      "Y_test_padded [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0 10  3 10 10  4 10 10 10 10 10 10 10  2 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10]\n"
     ]
    }
   ],
   "source": [
    "print('X_test_padded', X_test_padded[2001])\n",
    "print('Y_test_padded', Y_test_padded[2001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3684/3684 [==============================] - 3s 923us/step\n"
     ]
    }
   ],
   "source": [
    "ner_predictions = model.predict(X_test_padded, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3607002e-12 6.5027372e-12 9.9939620e-01 5.7736670e-05 2.3895378e-04\n",
      " 2.3150909e-05 1.4147745e-04 4.0447680e-06 1.0677781e-04 1.5955050e-06\n",
      " 2.9958817e-05]\n",
      "Length: 124.\n",
      "\n",
      "So output is still padded:\n",
      "(3684, 124, 11)\n"
     ]
    }
   ],
   "source": [
    "print(ner_predictions[2001][-15])\n",
    "print('Length: %d.' % len(ner_predictions[2001]))\n",
    "\n",
    "print('\\nSo output is still padded:')\n",
    "print(ner_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check with above:\n",
      "[5.3607002e-12 6.5027372e-12 9.9939620e-01 5.7736670e-05 2.3895378e-04\n",
      " 2.3150909e-05 1.4147745e-04 4.0447680e-06 1.0677781e-04 1.5955050e-06\n",
      " 2.9958817e-05]\n",
      "Length: 27. So padding removed.\n"
     ]
    }
   ],
   "source": [
    "ner_pred_num = []\n",
    "for pred_nbr, ner_preds in enumerate(ner_predictions):\n",
    "    # len(X_test_cat[pred_nbr]) is the length a sentence number pred_nbr.\n",
    "    # Since we used pre-paddding, extract tag predictions from -sentence_length to end.\n",
    "    ner_pred_num += [ ner_preds[ -len(X_test_cat[pred_nbr]) : ] ]\n",
    "\n",
    "print('Check with above:')\n",
    "print(ner_pred_num[2001][-15])\n",
    "print('Length: %d. So padding removed.' % len(ner_pred_num[2001]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From logistic regression to one tag\n",
    "We still have our results in probabilistic form from the softmax output.  \n",
    "Take the maximum to get the tag predicted as most likely.\n",
    "\n",
    "Note: `argmax` returns the **indices** of the maximum values along an axis.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'B-LOC', 3: 'B-MISC', 4: 'B-ORG', 5: 'B-PER', 6: 'I-LOC', 7: 'I-MISC', 8: 'I-ORG', 9: 'I-PER', 10: 'O'}\n"
     ]
    }
   ],
   "source": [
    "# Remember\n",
    "print(rev_ner_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length consistency check: 3684, 3684.\n",
      "\n",
      "Example for sentence in index 2001:\n",
      "\n",
      "Predicted:\n",
      " ['O', 'B-MISC', 'O', 'B-ORG', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Correct answer:\n",
      " ['O', 'B-MISC', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "ner_pred = []\n",
    "for sentence in ner_pred_num:\n",
    "    ner_pred_idx = list(map(np.argmax, sentence)) # Predicted indices for this sentence.\n",
    "    ner_pred_cat = list(map(rev_ner_idx.get, ner_pred_idx)) # Convert indices to 'I-ORG' and so on for this sentence.\n",
    "    ner_pred += [ner_pred_cat]\n",
    "\n",
    "print('Length consistency check: %d, %d.\\n' % (len(ner_pred), len(Y_test_cat)) )\n",
    "print('Example for sentence in index 2001:')\n",
    "print('\\nPredicted:\\n', ner_pred[2001])\n",
    "print('\\nCorrect answer:\\n', Y_test_cat[2001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 46666, correct 44898, accuracy 0.962114\n",
      "total unknown 1143, correct 960, accuracy 0.839895\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy separately for words in and not in dictionary.\n",
    "total, correct, total_ukn, correct_ukn = 0, 0, 0, 0\n",
    "for id_s, sentence in enumerate(X_test_cat):\n",
    "    for id_w, word in enumerate(sentence):\n",
    "        total += 1\n",
    "        if ner_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "            correct += 1\n",
    "        # The word is not in the dictionary\n",
    "        if word not in word_idx:\n",
    "            total_ukn += 1\n",
    "            if ner_pred[id_s][id_w] == Y_test_cat[id_s][id_w]:\n",
    "                correct_ukn += 1\n",
    "\n",
    "print('total %d, correct %d, accuracy %f' % \n",
    "      (total, correct, correct / total))\n",
    "if total_ukn != 0:\n",
    "    print('total unknown %d, correct %d, accuracy %f' % \n",
    "          (total_ukn, correct_ukn, correct_ukn / total_ukn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add predictions to file for evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ner_predictions(input_filepath, pred_sentence_list, output_filepath):\n",
    "    \"\"\"Add the NER predictions as a fifth column in test.txt.\"\"\"\n",
    "    with open(input_filepath, 'r') as readf, open(output_filepath, 'w') as writef:\n",
    "        sentence_idx, word_idx = 0, 0\n",
    "        for line in readf:\n",
    "            # If line was empty, next sentence.\n",
    "            if not line.strip():\n",
    "                writef.write('\\n')\n",
    "                sentence_idx += 1\n",
    "                word_idx = 0\n",
    "            else:\n",
    "                writef.write(line.rstrip() + ' ' + ner_pred[sentence_idx][word_idx] + '\\n')\n",
    "                word_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ner_predictions('test_copy.txt', ner_pred, 'test_result.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perl conlleval.pl < test_result.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
